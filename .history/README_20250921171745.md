# E-commerce Product Assistant

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11-blue?style=for-the-badge&logo=python&logoColor=white" alt="Python 3.11">
  <img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" alt="Streamlit">
  <img src="https://img.shields.io/badge/Astra%20DB-000000?style=for-the-badge&logo=datastax&logoColor=white" alt="Astra DB">
  <img src="https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge" alt="License: MIT">
</p>

An intelligent assistant designed to streamline e-commerce operations through automated data pipelines, AI-powered insights, and a user-friendly web interface.

## Overview

**ecomm-prod-assistant** is a full-stack solution built to empower e-commerce businesses by automating critical data management tasks. This project integrates a web scraper, ETL (Extract, Transform, Load) pipelines, and advanced AI modules to provide actionable insights into product trends, pricing strategies, and customer behavior.

The system is designed with a modular architecture, allowing for easy extension and maintenance. It leverages a modern Python stack including `uv` for package management, Streamlit for the user interface, and Astra DB for scalable data storage.

## Features

-   **Automated Data Ingestion:** Robust ETL pipelines to extract, transform, and load product data from various sources into a structured database.
-   **Interactive Scraper UI:** A user-friendly interface built with Streamlit to configure, initiate, and monitor web scraping jobs without writing code.
-   **AI-Powered Insights:** Leverages generative AI models (via Google Gemini and Groq APIs) to analyze product data, generate summaries, and identify market trends.
-   **Centralized Logging:** A global logging system to monitor application health, track pipeline progress, and simplify error diagnosis.
-   **Scalable Database Integration:** Utilizes Astra DB (a cloud-native Cassandra database) for high-performance, structured storage of large e-commerce datasets.
-   **Modular and Testable Code:** A clean project structure that separates concerns, making the codebase easy to test, maintain, and extend.

## Architecture

The project is composed of several key components that work together:

1.  **Scraper UI (Streamlit):** The user-facing entry point for initiating data collection. Users can specify target sites and scraping parameters.
2.  **ETL Pipeline:** A series of scripts that take the raw, scraped data, clean and transform it into a consistent format, and load it into the database.
3.  **Astra DB:** The core data store, providing a reliable and scalable foundation for all product and analytics data.
4.  **AI Assistant Module:** Connects to external AI APIs (Google Gemini, Groq) to perform analysis on the data stored in Astra DB.
5.  **Utilities & Logger:** Shared modules that provide helper functions (e.g., API clients, data connectors) and a centralized logging mechanism used across the entire application.

## Getting Started with `uv`

This project uses `uv` for high-performance package and virtual environment management.

### Prerequisites

-   Python 3.9+
-   `uv` (Python package manager)
-   Git
-   An [Astra DB](https://astra.datastax.com/) account
-   API keys for [Google Gemini](https://ai.google.dev/) and [Groq](https://console.groq.com/keys)

### Installation & Setup

1.  **Clone the Repository:**
    ```
    git clone https://github.com/nileshgode/ecomm-prod-assistant.git
    cd ecomm-prod-assistant
    ```

2.  **Install `uv` (if not already installed):**
    Follow the official instructions at [astral.sh/uv](https://astral.sh/uv). You can verify the installation with:
    ```
    uv --version
    ```

3.  **Create a Virtual Environment:**
    `uv` will create a `.venv` directory in your project folder.
    ```
    uv venv
    ```

4.  **Install Dependencies:**
    This command reads the `requirements.txt` file and installs all necessary packages into your virtual environment.
    ```
    uv pip install -r requirements.txt
    ```

5.  **Activate the Virtual Environment:**
    -   **Windows (PowerShell):**
        ```
        .venv\Scripts\Activate.ps1
        ```
    -   **Windows (CMD):**
        ```
        .venv\Scripts\activate.bat
        ```
    -   **macOS / Linux / Git Bash:**
        ```
        source .venv/bin/activate
        ```

6.  **Set Up Environment Variables:**
    Create a file named `.env` in the root directory and add your credentials:
    ```
    ASTRA_DB_TOKEN="your_astra_db_token"
    ASTRA_DB_KEYSPACE="your_keyspace_name"
    ASTRA_DB_API_ENDPOINT="your_api_endpoint_url"
    GOOGLE_API_KEY="your_google_api_key"
    GROQ_API_KEY="your_groq_api_key"
    ```

### Running the Application

With your virtual environment activated, you can now run the main application.

-   **Launch the Scraper UI:**
    ```
    streamlit run prod_assistant/scrapper_ui.py
    ```
    Open your browser and navigate to the local URL provided by Streamlit (usually `http://localhost:8501`).

-   **Run ETL Pipelines:**
    Execute the specific ETL scripts as needed from the command line.
    ```
    python prod_assistant/etl/your_etl_script.py
    ```
## Deployment to AWS

Once your application is running locally, you can deploy it to AWS to make it accessible online. Here are two common methods for deploying a containerized Streamlit application.

### Method 1: Deploying to AWS EC2 (Simple & Direct)

This method is straightforward and great for quickly getting your app online. It involves running the application on a single virtual server.

1.  **Launch an EC2 Instance:**
    *   From the AWS Management Console, launch a new EC2 instance (e.g., `t2.micro` or `t3.small` for a small app). Choose an Amazon Linux 2 or Ubuntu AMI.
    *   **Configure Security Group:** Create a security group that allows inbound traffic on:
        *   **Port 22 (SSH):** To connect to your instance.
        *   **Port 8501 (TCP):** The default port for Streamlit.

2.  **Connect to Your Instance:**
    Use SSH to connect to your newly created EC2 instance.
    ```
    ssh -i "your-key.pem" ec2-user@your-ec2-public-ip
    ```

3.  **Set Up the Environment:**
    *   Install Git, Python, and Docker on the instance.
    *   Clone your repository: `git clone https://github.com/nileshgode/ecomm-prod-assistant.git`
    *   Navigate into the project directory: `cd ecomm-prod-assistant`

4.  **Create `.env` File:**
    Create a `.env` file on the instance and populate it with your API keys and database credentials.
    ```
    nano .env
    ```

5.  **Run the Application using Docker (Recommended):**
    First, build a Docker image for your application. Create a `Dockerfile` in your project root:
    ```
    # Dockerfile
    FROM python:3.11-slim

    WORKDIR /app

    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    COPY . .

    EXPOSE 8501

    HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health

    ENTRYPOINT ["streamlit", "run", "prod_assistant/scrapper_ui.py", "--server.port=8501", "--server.address=0.0.0.0"]
    ```
    Now, build and run the container on your EC2 instance:
    ```
    docker build -t ecomm-assistant .
    docker run -d -p 8501:8501 --env-file .env ecomm-assistant
    ```

6.  **Access Your App:**
    You can now access your application by navigating to `http://<your-ec2-public-ip>:8501` in your web browser.

### Method 2: Deploying to AWS ECS with Fargate (Scalable & Robust)

This serverless approach is more scalable and resilient, making it ideal for production applications. It runs your Docker container without you needing to manage the underlying EC2 instance.

1.  **Containerize Your Application:**
    Ensure you have a `Dockerfile` as shown in the method above.

2.  **Push Docker Image to Amazon ECR (Elastic Container Registry):**
    *   Create a new private repository in ECR.
    *   Follow the "View push commands" instructions provided by AWS to build, tag, and push your Docker image from your local machine to ECR.

3.  **Set Up an ECS Cluster:**
    *   In the AWS ECS console, create a new cluster. Choose the "AWS Fargate (serverless)" infrastructure option.

4.  **Create a Task Definition:**
    *   This is a blueprint for your application. Define the container image to use (the one you pushed to ECR), CPU/memory allocation, and port mappings (port `8501`).
    *   In the "Environment" section, you can either add your environment variables directly or connect to **AWS Secrets Manager** for more secure credential handling.

5.  **Create an ECS Service:**
    *   Create a service within your cluster that runs your task definition.
    *   Configure the service to use a **load balancer** (Application Load Balancer) to distribute traffic to your app.
    *   Set up auto-scaling rules to automatically adjust the number of running tasks based on demand.

6.  **Access Your App:**
    Once the service is running, you can access your application through the DNS name of the Application Load Balancer. For a production setup, you would point a custom domain to this load balancer using Amazon Route 53.


## Contributing

Contributions are highly welcome! If you have suggestions for improvements, bug fixes, or new features, please follow these steps:

1.  Fork the repository.
2.  Create a new branch (`git checkout -b feature/your-feature-name`).
3.  Make your changes and commit them with a clear message.
4.  Push to the branch (`git push origin feature/your-feature-name`).
5.  Open a Pull Request.

## License

This project is licensed under the **MIT License**. See the `LICENSE` file for more details.

## Contact

For questions, feature requests, or bug reports, please open an issue in the repository.
